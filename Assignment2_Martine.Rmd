---
title: "Assignment2_Martine"
author: "Martine Lind Jensen"
date: "2024-02-22"
output: html_document
---

```{r Loading libraries}
pacman::p_load(tidyverse,
        here,
        posterior,
        cmdstanr,
        brms, 
        tidybayes,
        future, 
        purrr, 
        furrr)
```

Riccardos code: random agent to let the WSLS model play against 
```{r random agent from Riccardo to build models against}
trials <- 120
rate <- 0.5

# now as a function
RandomAgent_f <- function(input, rate){
  n <- length(input)
  choice <- rbinom(n, 1, rate)
  return(choice)
}
```

Our Win stay loose switch model with probabilistic error to either wins or losses. Such that high win prob means that you are not very likeli to shift when winning. And a low loss probability means that when you loose you are only 70% certain that you are gonna switch. So this is a way of coding the stubborness and on willingness to shift. We are interested in recovering these to parameters, win prob and loose prob.

##Simulating data
Here we simulate data from our WSLS agent against the random agent. 
```{r Asym WSLS agent}
# Building function for asymmetric win stay loose shift agent by including probabilities for winning and loosing

AsymWSLSAgent_f <- function(prevChoice, Feedback, winProb, lossProb){
  
  if (Feedback == 1){
    
    randomness <- rbinom(1, 1, winProb)
    # stay with choice agent would initially have made
    if (randomness == 1){
      choice = prevChoice 
    # randomly shift
    } else if (randomness == 0){
      choice = 1-prevChoice
    }
      
  } else if (Feedback == 0){
    
    randomness <- rbinom(1, 1, lossProb)
    # stay with choice agent would initially have made
    if (randomness == 1){
      choice = 1-prevChoice
    # randomly shift
    } else if (randomness == 0){
      choice = prevChoice
    }
  }
  
  return(choice)
}

# empty vectors for agents
Self <- rep(NA, trials)
Random <- rep(NA, trials)
Feedbacklist <- rep(NA, trials)

# other agent's choices
for (t in seq(trials)){
  Random[t] <- RandomAgent_f(trials, rate)
}

# first choice is random...
Self[1] <- rbinom(1,1,0.5)

# all other choices
for (t in 2:trials){
  
  # get feedback
  if (Self[t-1] == Random[t-1]){
    Feedback = 1
  } else {Feedback = 0}
  
  # register feedback
  Feedbacklist[t-1] <- Feedback
  
  # make decision
  Self[t] <- AsymWSLSAgent_f(Self[t-1], 
                         Feedback,
                         0.9, #Win probability
                         0.7) #Loose probability
}

if (Self[120] == Random[120]){
    Feedbacklist[120] = 1
  } else {Feedbacklist[120] = 0}

# turn into tibble
WSLS_df <- tibble(Self,
             Random,
             trial = seq(trials),
             feedback = Feedbacklist)

```

```{r}
## Create the data. N.B. note the two variables have different lengths: 1 for n, n for h.
data <- list(
  trials = 120, # 'trials' in data section of stan model
  choices = WSLS_df$Self, # 'choices' in data section of stan model
  feedback = WSLS_df$feedback # 'feedback' in data section of stan model
)
```

#Compiling model in stan 
This part is not done, just copied from Riccardo so that we know how to compile and run the model. 
```{r}
## Specify where the model is
file <- file.path("stan/WSLS.stan")

# Compile the model
mod <- cmdstan_model(file, 
                     # this specifies we can parallelize the gradient estimations on multiple cores
                     cpp_options = list(stan_threads = TRUE), 
                     # this is a trick to make it faster
                     stanc_options = list("O1")) 

# The following command calls Stan with specific options.
samples <- mod$sample(
  data = data, # the data :-)
  seed = 123,  # a seed, so I always get the same results
  chains = 2,  # how many chains should I fit (to check whether they give the same results)
  parallel_chains = 2, # how many of the chains can be run in parallel?
  threads_per_chain = 2, # distribute gradient estimations within chain across multiple cores
  iter_warmup = 1000,  # warmup iterations through which hyperparameters (steps and step length) are adjusted
  iter_sampling = 2000, # total number of iterations
  refresh = 0,  # how often to show that iterations have been run
  output_dir = "simmodels", # saves the samples as csv so it can be later loaded
  max_treedepth = 20, # how many steps in the future to check to avoid u-turns
  adapt_delta = 0.99, # how high a learning rate to adjust hyperparameters during warmup
)

# Save the fitted model
samples$save_object("simmodels/WSLS.rds")
```

```{r}
samples <- readRDS("simmodels/WSLS.rds")
```

```{r}
samples$cmdstan_diagnose()
```

```{r}
samples$summary() # summarize the model
```

```{r}
# Extract posterior samples and include sampling of the prior:
draws_df <- as_draws_df(samples$draws())
```

#Plots 
```{r}
ggplot(draws_df, 
       aes(.iteration,
           winprob, 
           group = .chain, 
           color = .chain)) +
  geom_line() +
  theme_classic()
```

```{r}
ggplot(draws_df, 
       aes(.iteration,
           lossprob, 
           group = .chain, 
           color = .chain)) +
  geom_line() +
  theme_classic()
```

```{r}
ggplot(draws_df) +
  geom_histogram(aes("prior_preds[1]"), color = "darkblue", fill = "blue", alpha = 0.3) +
  xlab("Predicted heads out of 120 trials") +
  ylab("Posterior Density") +
  theme_classic()
```


```{r}
#In logodds space
ggplot(draws_df) +
  geom_density(aes(lossprob), fill = "blue", alpha = 0.3) +
  geom_density(aes(lossprob_prior), fill = "red", alpha = 0.3) +
  xlab("LossProb") +
  ylab("Posterior Density") +
  theme_classic()

ggplot(draws_df) +
  geom_density(aes(winprob), fill = "blue", alpha = 0.3) +
  geom_density(aes(winprob_prior), fill = "red", alpha = 0.3) +
  xlab("WinProb") +
  ylab("Posterior Density") +
  theme_classic()

#Plotting in probability scale 
ggplot(draws_df) +
  geom_density(aes(inv_logit_scaled(winprob)), fill = "blue", alpha = 0.3) +
  geom_density(aes(inv_logit_scaled(winprob_prior)), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = 0.9, size = 2) + #true value from our simulation
  xlab("WinProb") +
  ylab("Posterior Density") +
  theme_classic()


ggplot(draws_df) +
  geom_density(aes(inv_logit_scaled(lossprob)), fill = "blue", alpha = 0.3) +
  geom_density(aes(inv_logit_scaled(lossprob_prior)), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = 0.7, size = 2) + #true value from our simulation
  xlab("LossProb") +
  ylab("Posterior Density") +
  theme_classic()
```

#Parameter recovery 
Parameter recovery setup for all values of winprob and lossprob between 0-1, with 0.1 increment. 
```{r}
#Setting up parallelization 
plan(multisession, workers = 4)

#Our parameter recovery contains recovery of winprob and lossprob by different random agent biases.

parameter_recovery <- function(seed, trials, winprob, lossprob, random_agent_bias) { 
  
    #Empty vectors for agents
    Self <- rep(NA, trials)
    Random <- rep(NA, trials)
    Feedbacklist <- rep(NA, trials)
    
    #Other agent's choices
    for (t in seq(trials)){
      Random[t] <- RandomAgent_f(trials, random_agent_bias)
    }
    
    #First choice is random
    Self[1] <- rbinom(1,1,0.5)
    
    #All other choices
    for (t in 2:trials){
      
      #Get feedback
      if (Self[t-1] == Random[t-1]){
        Feedback = 1
      } else {Feedback = 0}
      
      #Register feedback
      Feedbacklist[t-1] <- Feedback
      
      #Make decision
      Self[t] <- AsymWSLSAgent_f(Self[t-1], Feedback, winprob, lossprob) 
    }
    
    if (Self[120] == Random[120]){
        Feedbacklist[120] = 1
      } else {Feedbacklist[120] = 0}
    
    # turn into tibble
    temp <- tibble(Self,
                 Random,
                 trial = seq(trials),
                 feedback = Feedbacklist)
    
    #Data structure needed for implementing in the stan models 
    data <- list(
      trials = 120, 
      choices = temp$Self, 
      feedback = temp$feedback 
    )
    
    samples <- mod$sample(
      data = data,
      seed = 1000,
      chains = 1,
      parallel_chains = 1,
      threads_per_chain = 1,
      iter_warmup = 1000,
      iter_sampling = 2000,
      refresh = 0,
      max_treedepth = 20,
      adapt_delta = 0.99,
    )
    
    #Settting up datafram with as_draws
    draws_df <- as_draws_df(samples$draws())
    
    #Saving the variables we need to plot parameter recovery in temporary form to be returned
    temp <- tibble(winprobEst = inv_logit_scaled(draws_df$winprob),
                   lossprobEst = inv_logit_scaled(draws_df$lossprob),
                   winprobTrue = winprob, 
                   lossprobTrue = lossprob, 
                   random_agent_bias = random_agent_bias)
    
    return(temp)
  
}

#Setting up our variables for parameter recovery
sim_variables <- expand_grid(
  winprob = seq(0, 1, by = 0.1), 
  lossprob = seq(0, 1, by = 0.1), 
  r_a_b = seq(0, 0.5, by = 0.1)) %>% 
  mutate(
    seed = 1234, 
    trials = 120) 

#Doing fancy stuff with a fancy function to recover parameters 
recovery_df <- future_pmap_dfr(sim_variables, parameter_recovery, .options = furrr_options(seed = TRUE))

#Saving the dataframe
#write_csv(recovery_df, "recovery_try.csv")
```

```{r}
ggplot(recovery_df, aes(winprobTrue, winprobEst)) +
  geom_point(alpha = 0.1) +
  geom_smooth() +
  facet_wrap(.~random_agent_bias) +
  theme_classic()
```



















