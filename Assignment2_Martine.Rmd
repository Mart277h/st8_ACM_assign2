---
title: "Assignment2_Martine"
author: "Martine Lind Jensen"
date: "2024-02-22"
output: html_document
---

```{r Loading libraries}
pacman::p_load(tidyverse,
        here,
        posterior,
        cmdstanr,
        brms, 
        tidybayes,
        future, 
        purrr, 
        furrr)
```

Riccardos code: random agent to let the WSLS model play against 
```{r random agent from Riccardo to build models against}
trials <- 120
rate <- 0.5

# now as a function
RandomAgent_f <- function(input, rate){
  n <- length(input)
  choice <- rbinom(n, 1, rate)
  return(choice)
}
```

Our Win stay loose switch model with probabilistic error to either wins or losses. Such that high win prob means that you are not very likeli to shift when winning. And a low loss probability means that when you loose you are only 70% certain that you are gonna switch. So this is a way of coding the stubborness and on willingness to shift. We are interested in recovering these to parameters, win prob and loose prob.

##Simulating data
Here we simulate data from our WSLS agent against the random agent. 
```{r Asym WSLS agent}
# Building function for asymmetric win stay loose shift agent by including probabilities for winning and loosing

AsymWSLSAgent_f <- function(prevChoice, Feedback, winProb, lossProb){
  
  if (Feedback == 1){
    
    randomness <- rbinom(1, 1, winProb)
    # stay with choice agent would initially have made
    if (randomness == 1){
      choice = prevChoice 
    # randomly shift
    } else if (randomness == 0){
      choice = 1-prevChoice
    }
      
  } else if (Feedback == 0){
    
    randomness <- rbinom(1, 1, lossProb)
    # stay with choice agent would initially have made
    if (randomness == 1){
      choice = 1-prevChoice
    # randomly shift
    } else if (randomness == 0){
      choice = prevChoice
    }
  }
  
  return(choice)
}

# empty vectors for agents
Self <- rep(NA, trials)
Random <- rep(NA, trials)
Feedbacklist <- rep(NA, trials)

# other agent's choices
for (t in seq(trials)){
  Random[t] <- RandomAgent_f(trials, rate)
}

# first choice is random...
Self[1] <- rbinom(1,1,0.5)

# all other choices
for (t in 2:trials){
  
  # get feedback
  if (Self[t-1] == Random[t-1]){
    Feedback = 1
  } else {Feedback = 0}
  
  # register feedback
  Feedbacklist[t-1] <- Feedback
  
  # make decision
  Self[t] <- AsymWSLSAgent_f(Self[t-1], 
                         Feedback,
                         0.9, #Win probability
                         0.7) #Loose probability
}

#Cleaning data to fit stan
if (Self[120] == Random[120]){
    Feedbacklist[120] = 1
  } else {Feedbacklist[120] = 0}

# turn into tibble
WSLS_df <- tibble(Self,
             Random,
             trial = seq(trials),
             feedback = Feedbacklist)

```

```{r}
##Create the data
data <- list(
  trials = 120, 
  choices = WSLS_df$Self, 
  feedback = WSLS_df$feedback 
)
```

#Compiling model in stan 
 
```{r compiling and running the model}
# file path to model 
file <- file.path("stan/WSLS.stan")

# compiling the model (Riccardos way)
mod <- cmdstan_model(file, 
                     cpp_options = list(stan_threads = TRUE), # this specifies we can parallelize the gradient estimations on multiple cores
                     # this is a trick to make it faster
                     stanc_options = list("O1")) 

#Running the model
samples <- mod$sample(
  data = data, # data
  seed = 123, 
  chains = 2,  # how many chains should I fit (to check whether they give the same results)
  parallel_chains = 2, # how many of the chains can be run in parallel?
  threads_per_chain = 2, # distribute gradient estimations within chain across multiple cores
  iter_warmup = 1000,  # warmup iterations through which hyperparameters (steps and step length) are adjusted
  iter_sampling = 2000, # total number of iterations
  refresh = 0,  # how often to show that iterations have been run
  output_dir = "simmodels", # saves the samples as csv so it can be later loaded
  max_treedepth = 20, # how many steps in the future to check to avoid u-turns
  adapt_delta = 0.99, # how high a learning rate to adjust hyperparameters during warmup
)

# Save the fitted model
samples$save_object("simmodels/WSLS.rds")
```

```{r}
#Reading the model with the RDS
samples <- readRDS("simmodels/WSLS.rds")

samples$cmdstan_diagnose() #function checking the chains and stuff

samples$summary() # summarize the model

# Extract posterior samples and include sampling of the prior:
draws_df <- as_draws_df(samples$draws()) #as_draws is a fancy way of putting the data such that we can manipulate it easier
```

#Plots 
1. chain plots 
2. prior predictive check 
3. posterior predictive check 
4. prior posterior update checks 

```{r chain plots}
ggplot(draws_df, 
       aes(.iteration,
           winprob, 
           group = .chain, 
           color = .chain)) +
  geom_line() +
  theme_classic()

ggplot(draws_df, 
       aes(.iteration,
           lossprob, 
           group = .chain, 
           color = .chain)) +
  geom_line() +
  theme_classic()
```

```{r prior predictive choice}
ggplot(draws_df) +
  geom_histogram(aes(prior_preds_wp1_l0), color = "darkblue", fill = "blue", alpha = 0.3) +
  xlab("Predicted heads out of 120 trials") +
  ylab("Posterior Density") +
  theme_classic()

ggplot(draws_df) +
  geom_histogram(aes(prior_preds_wp0_l0), color = "darkblue", fill = "blue", alpha = 0.3) +
  xlab("Predicted heads out of 120 trials") +
  ylab("Posterior Density") +
  theme_classic()

ggplot(draws_df) +
  geom_histogram(aes(prior_preds_w0_lp1), color = "darkblue", fill = "blue", alpha = 0.3) +
  xlab("Predicted heads out of 120 trials") +
  ylab("Posterior Density") +
  theme_classic()

ggplot(draws_df) +
  geom_histogram(aes(prior_preds_w0_lp0), color = "darkblue", fill = "blue", alpha = 0.3) +
  xlab("Predicted heads out of 120 trials") +
  ylab("Posterior Density") +
  theme_classic()
```

```{r posterior predictive choice}
ggplot(draws_df) +
  geom_histogram(aes(post_preds_wp1_l0), color = "darkblue", fill = "blue", alpha = 0.3) +
  xlab("Predicted heads out of 120 trials") +
  ylab("Posterior Density") +
  theme_classic()

ggplot(draws_df) +
  geom_histogram(aes(post_preds_wp0_l0), color = "darkblue", fill = "blue", alpha = 0.3) +
  xlab("Predicted heads out of 120 trials") +
  ylab("Posterior Density") +
  theme_classic()

ggplot(draws_df) +
  geom_histogram(aes(post_preds_w0_lp1), color = "darkblue", fill = "blue", alpha = 0.3) +
  xlab("Predicted heads out of 120 trials") +
  ylab("Posterior Density") +
  theme_classic()

ggplot(draws_df) +
  geom_histogram(aes(post_preds_w0_lp0), color = "darkblue", fill = "blue", alpha = 0.3) +
  xlab("Predicted heads out of 120 trials") +
  ylab("Posterior Density") +
  theme_classic()

```

```{r prior posterior update checks}
#In logodds space
ggplot(draws_df) +
  geom_density(aes(lossprob), fill = "blue", alpha = 0.3) +
  geom_density(aes(lossprob_prior), fill = "red", alpha = 0.3) +
  xlab("LossProb") +
  ylab("Posterior Density") +
  theme_classic()

ggplot(draws_df) +
  geom_density(aes(winprob), fill = "blue", alpha = 0.3) +
  geom_density(aes(winprob_prior), fill = "red", alpha = 0.3) +
  xlab("WinProb") +
  ylab("Posterior Density") +
  theme_classic()

#Plotting in probability scale 
ggplot(draws_df) +
  geom_density(aes(inv_logit_scaled(winprob)), fill = "blue", alpha = 0.3) +
  geom_density(aes(inv_logit_scaled(winprob_prior)), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = 0.9, size = 2) + #true value from our simulation
  xlab("WinProb") +
  ylab("Posterior Density") +
  theme_classic()


ggplot(draws_df) +
  geom_density(aes(inv_logit_scaled(lossprob)), fill = "blue", alpha = 0.3) +
  geom_density(aes(inv_logit_scaled(lossprob_prior)), fill = "red", alpha = 0.3) +
  geom_vline(xintercept = 0.7, size = 2) + #true value from our simulation
  xlab("LossProb") +
  ylab("Posterior Density") +
  theme_classic()
```

#Parameter recovery 
Parameter recovery setup for all values of winprob and lossprob between 0-1, with 0.1 increment. 
```{r}
#Setting up parallelization 
plan(multisession, workers = 4)

#Our parameter recovery contains recovery of winprob and lossprob by different random agent biases.

parameter_recovery <- function(seed, trials, winprob, lossprob, r_a_b) { 
  
    #Empty vectors for agents
    Self <- rep(NA, trials)
    Random <- rep(NA, trials)
    Feedbacklist <- rep(NA, trials)
    
    #Other agent's choices
    for (t in seq(trials)){
      Random[t] <- RandomAgent_f(trials, r_a_b)
    }
    
    #First choice is random
    Self[1] <- rbinom(1,1,0.5)
    
    #All other choices
    for (t in 2:trials){
      
      #Get feedback
      if (Self[t-1] == Random[t-1]){
        Feedback = 1
      } else {Feedback = 0}
      
      #Register feedback
      Feedbacklist[t-1] <- Feedback
      
      #Make decision
      Self[t] <- AsymWSLSAgent_f(Self[t-1], Feedback, winprob, lossprob) 
    }
    
    if (Self[120] == Random[120]){
        Feedbacklist[120] = 1
      } else {Feedbacklist[120] = 0}
    
    # turn into tibble
    temp <- tibble(Self,
                 Random,
                 trial = seq(trials),
                 feedback = Feedbacklist)
    
    #Data structure needed for implementing in the stan models 
    data <- list(
      trials = 120, 
      choices = temp$Self, 
      feedback = temp$feedback 
    )
    
    samples <- mod$sample(
      data = data,
      seed = 1000,
      chains = 1,
      parallel_chains = 1,
      threads_per_chain = 1,
      iter_warmup = 1000,
      iter_sampling = 2000,
      refresh = 0,
      max_treedepth = 20,
      adapt_delta = 0.99,
    )
    
    #Settting up datafram with as_draws
    draws_df <- as_draws_df(samples$draws())
    
    #Saving the variables we need to plot parameter recovery in temporary form to be returned
    temp <- tibble(winprobEst = inv_logit_scaled(draws_df$winprob),
                   lossprobEst = inv_logit_scaled(draws_df$lossprob),
                   winprobTrue = winprob, 
                   lossprobTrue = lossprob, 
                   r_a_b = r_a_b)
    
    return(temp)
  
}

#Setting up our variables for parameter recovery
sim_variables <- expand_grid(
  winprob = seq(0, 1, by = 0.1), 
  lossprob = seq(0, 1, by = 0.1), 
  r_a_b = seq(0.1, 0.5, by = 0.1)) %>% 
  mutate(
    seed = 1234, 
    trials = 120) 

#Doing fancy stuff with a fancy function to recover parameters 
recovery_df <- future_pmap_dfr(sim_variables, parameter_recovery, .options = furrr_options(seed = TRUE))

#Saving the dataframe
write_csv(recovery_df, "recovery_try.csv")
```

```{r plotting parameter recovery}
recover_plot <- ggplot(recovery_df, aes(winprobTrue, winprobEst)) +
  geom_point(alpha = 0.1) +
  geom_smooth() +
  facet_wrap(.~r_a_b) +
  theme_classic()
```



















